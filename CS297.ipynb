{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27dd27c1-a522-429d-8a61-9780ed842a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow<2.19,>=2.18\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0\n",
      "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes<0.5.0,>=0.4.0\n",
      "  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras>=3.5.0\n",
      "  Downloading keras-3.9.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (65.5.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m137.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.70.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.19,>=2.18\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m137.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Collecting optree\n",
      "  Downloading optree-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (395 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich\n",
      "  Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, rich, keras, tensorflow, tf-keras\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.13.0 keras-3.9.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.1 protobuf-5.29.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 tf-keras-2.18.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-hgi5ej5k/flash-attn_f3e20b5c50804fc08eef7864e2f1d5e2/setup.py\", line 22, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (4.2.2)\n",
      "Collecting click!=8.0.0,>=7.1\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<3,>=2.6\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 pydantic-2.10.6 pydantic-core-2.27.2 sentry-sdk-2.22.0 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate>=0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Collecting torch>=2.0.0\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: psutil in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from accelerate>=0.26.0) (6.0.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.21.0\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.1/468.1 kB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.2.0-py3-none-any.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.2.0\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.4.127\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.7.4)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, tqdm, sympy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12, torch, accelerate\n",
      "Successfully installed accelerate-1.4.0 filelock-3.17.0 fsspec-2025.2.0 huggingface-hub-0.29.2 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 safetensors-0.5.3 sympy-1.13.1 torch-2.6.0 tqdm-4.67.1 triton-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Installing collected packages: regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 tokenizers-0.21.0 transformers-4.49.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.24.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (0.29.2)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.11.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m937.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (24.1)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m371.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m228.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from datasets) (2.0.2)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.9/507.9 kB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.8/346.8 kB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras \n",
    "!pip install flash-attn \n",
    "!pip install wandb \n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install transformers \n",
    "!pip install datasets \n",
    "\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"d687aedb7a16ede6abad66b2adade95edf09f216\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4583b892-4cdc-47c1-bfdd-4d4c1e7976d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715b73f8-6e41-4c25-be7b-d5d5054148c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_boxed_answer(text):\n",
    "    match = re.search(\n",
    "        r'\\\\boxed{\\s*(-?\\d+(?:\\.\\d+)?|-\\s*\\\\frac{\\s*(\\d+)\\s*}{\\s*(\\d+)\\s*}|\\\\frac{\\s*(\\d+)\\s*}{\\s*(\\d+)\\s*})\\s*}', \n",
    "        text\n",
    "    )\n",
    "    if match:\n",
    "        if match.group(2) and match.group(3):  # Negative fraction case (-\\frac{a}{b})\n",
    "            return f\"-{match.group(2)}/{match.group(3)}\"\n",
    "        elif match.group(4) and match.group(5):  # Positive fraction case (\\frac{a}{b})\n",
    "            return f\"{match.group(4)}/{match.group(5)}\"\n",
    "        else:  # Whole number or decimal case\n",
    "            return match.group(1).replace(\" \", \"\")  # Remove spaces for consistency\n",
    "\n",
    "    return None\n",
    "\n",
    "import re\n",
    "from fractions import Fraction\n",
    "\n",
    "def extract_last_number(text):\n",
    "    text = text.replace('$', '').replace('%', '')\n",
    "    pattern = r'(-?\\d+\\.\\d+|-?\\d+/\\d+|-?\\d+)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if matches:\n",
    "        last_match = matches[-1]\n",
    "        if '/' in last_match:\n",
    "            return float(Fraction(last_match))\n",
    "        else:\n",
    "            return float(last_match)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e39f2a3-0fc0-4342-b391-d5c3ec137e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = r\"\"\"<｜begin_of_sentence｜>  \n",
    "Below is a Math problem that is on the difficulty level of national olympiads. \n",
    "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
    "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
    "although some problems are slightly easier and some are slightly harder. \n",
    "The problems are all in LaTeX format. \n",
    "Answers may require basic computations, e.g., square roots, absolute values. \n",
    "Provide the final answer in the end as a numerical value.\n",
    "This is mandatory and you will be punished for not doing so.\n",
    "Now here is your question: \n",
    "### Problem <｜User｜> {} -------------- \n",
    "Provide the answer here: \n",
    "### Answer <｜Assistant｜> <think> {}  answer: {} </think> \"\"\"\n",
    "\n",
    "# r\"\"\"<｜begin_of_sentence｜>  Below is a Math problem that is on the difficulty level of national olympiads. You are a international gold medalist in Math, so you should be able to solve this problem. You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, although some problems are slightly easier and some are slightly harder. The problems are all in LaTeX format. Answers may require basic computations, e.g., square roots, absolute values. Provide the final answer inside $\\boxed{{}}. Example: $\\boxed{{9}}, $\\boxed{\\frac{13}{6}}, $\\boxed{-7}. This is mandatory and you will be punished for not doing so. Now here is your question: ### Problem <｜User｜> {} -------------- Provide the answer here: ### Answer <｜Assistant｜> <think> {} </think> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "830e30b0-4e68-4657-9a39-65e3728ea073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b1d6bcff744b518d11211f5d3617e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b29ee44d7e047d29ec1355002598747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00005.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f2a8cbe7d845229ab6a39dfa40e6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00001-of-00005.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6655ab354ce14c55a050cd22ce9a5970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00002-of-00005.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ad627d1f0e475a9c82baae244a29fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00003-of-00005.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a770edaf3c244f1a44af35c425c114b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00004-of-00005.parquet:   0%|          | 0.00/247M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8f660917dc432daf9f20bc0b75b1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/166k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c13c4eb68749969f53224eac2ac595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/859494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed24f9dfda4944f3a9ea3c16b8cafcb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 223313\n",
      "count 29\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = \"<｜end_of_sentence｜>\"\n",
    "def formatting_prompts_func(examples, split):\n",
    "    problems = examples[\"problem\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    texts = []\n",
    "    count = 0\n",
    "    if split == \"train\":\n",
    "        for problem, solution in zip(problems, solutions):\n",
    "            if solution.count(\"$\\\\boxed{\") == 1:\n",
    "                answer = extract_boxed_answer(solution)\n",
    "                if answer is not None:\n",
    "                    text = SYSTEM_PROMPT.format(problem, solution, answer) + EOS_TOKEN\n",
    "                    texts.append({\"text\" : text})\n",
    "                    count = count + 1\n",
    "    elif split == \"test\":\n",
    "        for problem, solution in zip(problems, solutions):\n",
    "            if solution.count(\"$\\\\boxed{\") == 1:\n",
    "                answer = extract_boxed_answer(solution)\n",
    "                if answer is not None:\n",
    "                    text = SYSTEM_PROMPT.format(problem, \"\", \"\") + EOS_TOKEN\n",
    "                    texts.append({\"text\" : text, \"answer\": answer})\n",
    "                    count = count + 1\n",
    "    print(\"count\", count)\n",
    "    return texts\n",
    "\n",
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"AI-MO/NuminaMath-CoT\", split = \"train\")\n",
    "test_dataset = load_dataset(\"AI-MO/NuminaMath-CoT\", split = \"test\")\n",
    "combined_dataset = concatenate_datasets([train_dataset, test_dataset])\n",
    "\n",
    "shuffled_dataset = combined_dataset.shuffle(seed=42)\n",
    "final_dataset = shuffled_dataset.train_test_split(test_size=0.0001)\n",
    "# print(type(train_dataset))\n",
    "\n",
    "train_dataset = formatting_prompts_func(final_dataset[\"train\"], \"train\")\n",
    "test_dataset = formatting_prompts_func(final_dataset[\"test\"], \"test\")\n",
    "# test_dataset = load_dataset(\"AI-MO/NuminaMath-CoT\", split = \"test\")\n",
    "# test_dataset = formatting_prompts_func(test_dataset, \"test\")\n",
    "\n",
    "# all_data = train_dataset + test_dataset\n",
    "# random.shuffle(all_data)\n",
    "# size_of_eval_data = 22334\n",
    "# train_dataset = all_data[:size_of_eval_data]\n",
    "# test_dataset = all_data[size_of_eval_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158ac1c0-43db-4c93-9674-480de5de27cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = len(eval_examples)\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   for example in eval_examples:\n",
    "       full_prompt = example[\"text\"]\n",
    "       expected = example[\"answer\"]\n",
    "\n",
    "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "       with torch.no_grad():\n",
    "           outputs = model.generate(\n",
    "               inputs,\n",
    "               max_new_tokens=32768,\n",
    "               temperature=0.7,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               forced_eos_token_id=tokenizer.eos_token_id,\n",
    "               early_stopping=False,\n",
    "           )\n",
    "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "       try:\n",
    "           predicted = extract_boxed_answer(response)\n",
    "           if predicted == expected:  \n",
    "               is_correct = True\n",
    "           else:\n",
    "               pred_num = extract_last_number(response)\n",
    "               is_correct = (pred_num is not None and expected is not None and\n",
    "                           pred_num == expected)\n",
    "\n",
    "           if is_correct:\n",
    "               correct += 1\n",
    "           print(\"\\nResponse\")\n",
    "           print(response)\n",
    "           print(\"\\nExpected Answer:\")\n",
    "           print(expected)\n",
    "           print(\"\\nExtracted Answer:\")\n",
    "           print(predicted)\n",
    "           print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "           print(\"-\"*50)\n",
    "\n",
    "       except Exception as e:\n",
    "           print(\"\\nFailed to parse model output for prompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"Error:\", e)\n",
    "           print(\"-\"*50)\n",
    "\n",
    "   accuracy = (correct / total) * 100\n",
    "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   model.train()\n",
    "   return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee46d399-0b5e-4ce8-9d72-7d4b514603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correctness_reward(prompts, completions, answer, **kwargs):\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   extracted = [extract_boxed_answer(r) for r in responses]\n",
    "   rewards = []\n",
    "   for r, a in zip(extracted, answer):\n",
    "       if r == a:  \n",
    "           rewards.append(2.0)\n",
    "       else:\n",
    "           r_num = extract_last_number(str(r))\n",
    "           a_num = extract_last_number(str(a))\n",
    "           if r_num is not None and a_num is not None and r_num == a_num:\n",
    "               rewards.append(1.5)\n",
    "           else:\n",
    "               rewards.append(0.0)\n",
    "   completion_lengths = [len(response.split()) for response in responses]\n",
    "   return rewards\n",
    "\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "   responses = [completion[0]['content'] for completion in completions]\n",
    "   rewards = []\n",
    "   format_scores = []\n",
    "   for response in responses:\n",
    "       score = 0.0\n",
    "       if \"<think>\" in response: score += 0.2\n",
    "       if \"</think>\" in response: score += 0.2\n",
    "       if \"\\boxed\" in response: score += 0.2\n",
    "       rewards.append(score)\n",
    "       format_scores.append(score)\n",
    "   return rewards\n",
    "\n",
    "\n",
    "def combined_reward(prompts, completions, answer):\n",
    "   correctness_scores = correctness_reward(prompts=prompts, completions=completions, answer=answer)\n",
    "   format_scores = format_reward(completions=completions)\n",
    "\n",
    "   combined_rewards = []\n",
    "   for c_score, f_score in zip(correctness_scores, format_scores):\n",
    "       # Correctness score range: 0.0 to 2.0\n",
    "       # Format score range: 0.0 to 0.6\n",
    "       # Total range: 0.0 to 2.6\n",
    "       combined_rewards.append(c_score + f_score)\n",
    "\n",
    "   return combined_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68a7120-a066-48cf-89be-cb82f14752e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, input_ids):\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_log_probs(model, input_ids, attention_mask, logits_to_keep):\n",
    "    logits = model(input_ids=input_ids, attention_mask=attention_mask).logits[:, :-1, :]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, tokenizer, prompts, num_generations=4, max_completion_length=32768):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"].to(device)\n",
    "    prompt_mask = inputs[\"attention_mask\"].to(device)\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}, Device before model: {prompt_ids.device}\")\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "    outputs = model.generate(\n",
    "        prompt_ids,\n",
    "        attention_mask=prompt_mask,\n",
    "        max_new_tokens=max_completion_length,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=False\n",
    "    )\n",
    "    print(f\"Output batch size: {outputs.size(0)}, Device after model: {outputs.device}\")\n",
    "    completion_ids = outputs[:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask\n",
    "\n",
    "def generate_rollout_data(model, ref_model, tokenizer, batch_samples, num_generations, max_completion_length):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompts = [sample[\"text\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [extract_boxed_answer(sample[\"text\"]) if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, tokenizer, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    formatted_completions = [[{'content': tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }\n",
    "\n",
    "def grpo_loss(model, ref_model, rollout_data, tokenizer, reward_function, beta=0.01, epsilon=0.2):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs)\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    #print(f\"Rewards: {rewards}\")  # Debug rewards\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    rewards = rewards.view(batch_size, num_generations)\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward\n",
    "\n",
    "def train_with_grpo(model, tokenizer, train_data, num_iterations=1, num_steps=500, batch_size=4,\n",
    "                              num_generations=4, max_completion_length=128, beta=0.1,\n",
    "                              learning_rate=5e-6, mu=3, epsilon=0.2, reward_function=None, device_ids=None):\n",
    "    assert device_ids is not None and len(device_ids) > 1, \"This code needs at least 2 GPU cores to run!\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "    print(f\"Model wrapped with DataParallel across GPUs: {device_ids}\")\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "\n",
    "        ref_model = copy.deepcopy(model.module)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Reference model created.\")\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            batch_samples = random.sample(train_data, batch_size)\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    tokenizer,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model.module,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    tokenizer,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "                for i in range(torch.cuda.device_count()):\n",
    "                   print(f\"GPU {i} Usage: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MiB, \"\n",
    "                         f\"Utilization: {torch.cuda.utilization(i)}%\")\n",
    "    return model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5429d7-e55e-42d9-b27a-87704edd3ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using primary device: cuda:0\n",
      "Model downloaded\n",
      "Detected 8 GPUs\n",
      "{'text': '<｜begin_of_sentence｜>  \\nBelow is a Math problem that is on the difficulty level of national olympiads. \\nYou are a international gold medalist in Math, so you should be able to solve this problem. \\nYou should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \\nalthough some problems are slightly easier and some are slightly harder. \\nThe problems are all in LaTeX format. \\nAnswers may require basic computations, e.g., square roots, absolute values. \\nProvide the final answer in the end as a numerical value.\\nThis is mandatory and you will be punished for not doing so.\\nNow here is your question: \\n### Problem <｜User｜> Pedro goes to the market and buys a total of 32 plums and peaches for 52 dollars. A plum costs 2 dollars and a peach costs 1 dollar. How many plums did Pedro buy? -------------- \\nProvide the answer here: \\n### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜>', 'answer': '20'}\n",
      "\n",
      "Initial model evaluation before finetuning:\n",
      "\n",
      "==================================================\n",
      "EVALUATION ON 29 EXAMPLES\n",
      "==================================================\n",
      "\n",
      "Response\n",
      "<｜begin_of_sentence｜>  \n",
      "Below is a Math problem that is on the difficulty level of national olympiads. \n",
      "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
      "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
      "although some problems are slightly easier and some are slightly harder. \n",
      "The problems are all in LaTeX format. \n",
      "Answers may require basic computations, e.g., square roots, absolute values. \n",
      "Provide the final answer in the end as a numerical value.\n",
      "This is mandatory and you will be punished for not doing so.\n",
      "Now here is your question: \n",
      "### Problem <｜User｜> Pedro goes to the market and buys a total of 32 plums and peaches for 52 dollars. A plum costs 2 dollars and a peach costs 1 dollar. How many plums did Pedro buy? -------------- \n",
      "Provide the answer here: \n",
      "### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜>  \n",
      "Alright, let's try to figure out how many plums Pedro bought. So, Pedro went to the market and bought a total of 32 plums and peaches for 52 dollars. A plum costs 2 dollars, and a peach costs 1 dollar. We need to find out how many plums he bought.\n",
      "\n",
      "Okay, let's break this down. Let's say the number of plums Pedro bought is P, and the number of peaches is E. So, we have two pieces of information here:\n",
      "\n",
      "1. The total number of fruits he bought is 32. So, P + E = 32.\n",
      "2. The total cost of these fruits is 52 dollars. Since each plum costs 2 dollars and each peach costs 1 dollar, the total cost equation would be 2P + E = 52.\n",
      "\n",
      "Now, we have a system of two equations:\n",
      "\n",
      "1. P + E = 32\n",
      "2. 2P + E = 52\n",
      "\n",
      "Hmm, I think I can solve this using substitution or elimination. Let me try elimination. If I subtract the first equation from the second equation, I can eliminate E.\n",
      "\n",
      "So, subtracting equation 1 from equation 2:\n",
      "\n",
      "(2P + E) - (P + E) = 52 - 32  \n",
      "2P + E - P - E = 20  \n",
      "P = 20\n",
      "\n",
      "Wait, so P equals 20? That means Pedro bought 20 plums. But let me double-check that because sometimes when we subtract equations, we might make a mistake.\n",
      "\n",
      "Let me plug P = 20 back into the first equation to find E.\n",
      "\n",
      "P + E = 32  \n",
      "20 + E = 32  \n",
      "E = 32 - 20  \n",
      "E = 12\n",
      "\n",
      "So, if Pedro bought 20 plums, he bought 12 peaches. Let me verify if that's correct with the total cost.\n",
      "\n",
      "20 plums at 2 dollars each would be 20 * 2 = 40 dollars. 12 peaches at 1 dollar each would be 12 * 1 = 12 dollars. Adding those together, 40 + 12 = 52 dollars. Perfect, that matches the total cost given.\n",
      "\n",
      "Therefore, Pedro bought 20 plums. I think that makes sense. But just to be thorough, let me try solving the system using substitution as well to see if I get the same answer.\n",
      "\n",
      "From the first equation, P + E = 32, we can express E as E = 32 - P.\n",
      "\n",
      "Now, plug this expression for E into the second equation:\n",
      "\n",
      "2P + E = 52  \n",
      "2P + (32 - P) = 52  \n",
      "2P + 32 - P = 52  \n",
      "(2P - P) + 32 = 52  \n",
      "P + 32 = 52  \n",
      "P = 52 - 32  \n",
      "P = 20\n",
      "\n",
      "Okay, so again, I get P = 20. That confirms my earlier result. So, Pedro bought 20 plums.\n",
      "\n",
      "I think I've covered all the bases here. I set up the equations correctly, solved them both by elimination and substitution, and verified the answer by plugging it back into the original problem. Everything checks out, so I'm confident that the answer is correct.\n",
      "</think>\n",
      "\n",
      "Pedro bought 20 plums. \n",
      "\n",
      "Answer: 20\n",
      "\n",
      "Expected Answer:\n",
      "20\n",
      "\n",
      "Extracted Answer:\n",
      "None\n",
      "\n",
      "Correct: ✗\n",
      "--------------------------------------------------\n",
      "\n",
      "Response\n",
      "<｜begin_of_sentence｜>  \n",
      "Below is a Math problem that is on the difficulty level of national olympiads. \n",
      "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
      "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
      "although some problems are slightly easier and some are slightly harder. \n",
      "The problems are all in LaTeX format. \n",
      "Answers may require basic computations, e.g., square roots, absolute values. \n",
      "Provide the final answer in the end as a numerical value.\n",
      "This is mandatory and you will be punished for not doing so.\n",
      "Now here is your question: \n",
      "### Problem <｜User｜> Let $m$ be the largest four-digit number in base-5. Convert $m$ to a base-7 number. -------------- \n",
      "Provide the answer here: \n",
      "### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜>  \n",
      "Alright, so I've got this problem here: I need to find the largest four-digit number in base-5, convert that number to base-7, and then provide the answer. Let me break it down step by step. \n",
      "\n",
      "First, let's understand what a four-digit number in base-5 looks like. In any base, a four-digit number ranges from 0000 to 4444 in base-5. Since we're looking for the largest four-digit number, that should be 4444 in base-5. Got that part down. \n",
      "\n",
      "Now, I need to convert this base-5 number, which is 4444, into base-7. Hmm, how do I do that? Well, I remember that converting from one base to another usually involves understanding the place values. In base-5, each digit represents powers of 5, right? So, the rightmost digit is 5^0, then 5^1, 5^2, and so on. Similarly, in base-7, each digit is a power of 7. \n",
      "\n",
      "So, the first step is to figure out what 4444 in base-5 is in decimal (base-10), and then convert that decimal number to base-7. Let me start by converting 4444 from base-5 to base-10.\n",
      "\n",
      "To do that, I'll expand each digit multiplied by the corresponding power of 5. The number 4444 in base-5 can be broken down as:\n",
      "\n",
      "- The first 4 is in the 5^3 place.\n",
      "- The second 4 is in the 5^2 place.\n",
      "- The third 4 is in the 5^1 place.\n",
      "- The fourth 4 is in the 5^0 place.\n",
      "\n",
      "So, in mathematical terms, that's:\n",
      "\n",
      "4 * 5^3 + 4 * 5^2 + 4 * 5^1 + 4 * 5^0\n",
      "\n",
      "Let me compute each of these terms:\n",
      "\n",
      "First, 5^3 is 125, so 4 * 125 = 500.\n",
      "\n",
      "Next, 5^2 is 25, so 4 * 25 = 100.\n",
      "\n",
      "Then, 5^1 is 5, so 4 * 5 = 20.\n",
      "\n",
      "Finally, 5^0 is 1, so 4 * 1 = 4.\n",
      "\n",
      "Now, adding all these together: 500 + 100 + 20 + 4. Let's do the addition step by step.\n",
      "\n",
      "500 + 100 is 600.\n",
      "\n",
      "600 + 20 is 620.\n",
      "\n",
      "620 + 4 is 624.\n",
      "\n",
      "So, 4444 in base-5 is equal to 624 in decimal.\n",
      "\n",
      "Now, I need to convert 624 from decimal to base-7. How do I do that? I remember that you can do this by dividing the number by the base and keeping track of the remainders.\n",
      "\n",
      "Let me outline the steps:\n",
      "\n",
      "1. Divide 624 by 7. The quotient will be the next number, and the remainder will be the next digit in the base-7 number.\n",
      "2. Repeat the process with the quotient until it becomes zero.\n",
      "3. The base-7 number is then the remainders read from bottom to top.\n",
      "\n",
      "Alright, let's apply this.\n",
      "\n",
      "First division: 624 ÷ 7.\n",
      "\n",
      "7 goes into 62 four times (since 7*4=28), but wait, actually, let me compute 7*89=623, which is just one less than 624. So, 7*89=623, so the quotient is 89, and the remainder is 1.\n",
      "\n",
      "Wait, let me verify that. 7*89: 7*90 is 630, so 7*89 is 630 - 7 = 623. Yes, that's correct. So, 624 - 623 is 1. So, the remainder is 1.\n",
      "\n",
      "So, the first remainder is 1, and the quotient is 89.\n",
      "\n",
      "Next, take 89 and divide by 7.\n",
      "\n",
      "7 goes into 8 once (7*1=7), but let me do a more precise calculation. 7*12=84, which is less than 89. 7*13=91, which is too big. So, 12 times with a remainder.\n",
      "\n",
      "Wait, 7*12=84, so 89-84=5. So, the remainder is 5, and the quotient is 12.\n",
      "\n",
      "So, next remainder is 5, quotient is 12.\n",
      "\n",
      "Next, take 12 and divide by 7.\n",
      "\n",
      "7 goes into 12 once (7*1=7), and the remainder is 12 - 7=5. So, remainder is 5, quotient is 1.\n",
      "\n",
      "So, next remainder is 5, quotient is 1.\n",
      "\n",
      "Now, take 1 and divide by 7.\n",
      "\n",
      "7 goes into 1 zero times, with a remainder of 1. So, remainder is 1, quotient is 0.\n",
      "\n",
      "Since the quotient is now zero, we stop.\n",
      "\n",
      "Now, compiling the remainders from last to first, the remainders we obtained were: 1 (from the last division), 5, 5, 1.\n",
      "\n",
      "Wait, no. Let me make sure: when you perform the division, each remainder is the next digit in the base-7 number, starting from the least significant digit. So, the first remainder we got was 1 (from dividing 624 by 7), then 5 (from dividing 89 by 7), then 5 (from dividing 12 by 7), and finally 1 (from dividing 1 by 7). \n",
      "\n",
      "So, reading the remainders from last to first, we get 1, 5, 5, 1. So, putting that together, 1 5 5 1 in base-7.\n",
      "\n",
      "Wait, but let me double-check. Let me convert 1551 (base-7) back to decimal to ensure it's 624.\n",
      "\n",
      "Compute 1*7^3 + 5*7^2 + 5*7^1 + 1*7^0.\n",
      "\n",
      "Compute each term:\n",
      "\n",
      "1*343 = 343.\n",
      "\n",
      "5*49 = 245.\n",
      "\n",
      "5*7 = 35.\n",
      "\n",
      "1*1 = 1.\n",
      "\n",
      "Add them up: 343 + 245 is 588.\n",
      "\n",
      "588 + 35 is 623.\n",
      "\n",
      "623 + 1 is 624.\n",
      "\n",
      "Yes, that's correct. So, 1551 in base-7 is 624 in decimal, which matches our previous conversion. So, that seems to check out.\n",
      "\n",
      "Alternatively, to ensure that I didn't make a mistake in the division steps, let me try a different approach.\n",
      "\n",
      "Another way is to express 624 in base-7 by finding how many times each power of 7 fits into 624.\n",
      "\n",
      "Let's list the powers of 7:\n",
      "\n",
      "7^0 = 1\n",
      "\n",
      "7^1 = 7\n",
      "\n",
      "7^2 = 49\n",
      "\n",
      "7^3 = 343\n",
      "\n",
      "7^4 = 2401\n",
      "\n",
      "But 2401 is larger than 624, so we don't need to go beyond 7^3.\n",
      "\n",
      "So, starting with 7^3 = 343.\n",
      "\n",
      "How many times does 343 go into 624? Well, 343*1=343, 343*2=686, which is too big. So, it goes in once.\n",
      "\n",
      "So, the coefficient for 7^3 is 1.\n",
      "\n",
      "Subtract 343 from 624: 624 - 343 = 281.\n",
      "\n",
      "Now, moving to 7^2 = 49.\n",
      "\n",
      "How many times does 49 go into 281? Let's see: 49*5=245, 49*6=294 which is too big. So, 5 times.\n",
      "\n",
      "So, coefficient for 7^2 is 5.\n",
      "\n",
      "Subtract 245 from 281: 281 - 245 = 36.\n",
      "\n",
      "Next, 7^1 = 7.\n",
      "\n",
      "How many times does 7 go into 36? 7*5=35, which is less than 36, and 7*6=42 which is too big. So, 5 times.\n",
      "\n",
      "So, coefficient for 7^1 is 5.\n",
      "\n",
      "Subtract 35 from 36: 36 - 35 = 1.\n",
      "\n",
      "Finally, 7^0 = 1.\n",
      "\n",
      "How many times does 1 go into 1? Just once.\n",
      "\n",
      "So, coefficient for 7^0 is 1.\n",
      "\n",
      "Putting it all together, the coefficients are 1 (for 7^3), 5 (for 7^2), 5 (for 7^1), and 1 (for 7^0). So, that's 1 5 5 1 in base-7, which matches what I got earlier.\n",
      "\n",
      "Therefore, the base-7 representation of 624 (which is 4444 in base-5) is 1551 in base-7.\n",
      "\n",
      "So, the final answer is 1551.\n",
      "\n",
      "**Final Answer**\n",
      "The base-7 number is \\boxed{1551}.\n",
      "</think>\n",
      "\n",
      "To find the largest four-digit number in base-5, we note that it is 4444 in base-5. We need to convert this number to base-7.\n",
      "\n",
      "First, convert 4444 from base-5 to decimal (base-10):\n",
      "\n",
      "\\[\n",
      "4 \\times 5^3 + 4 \\times 5^2 + 4 \\times 5^1 + 4 \\times 5^0\n",
      "\\]\n",
      "\n",
      "Calculating each term:\n",
      "\n",
      "\\[\n",
      "4 \\times 125 = 500\n",
      "\\]\n",
      "\\[\n",
      "4 \\times 25 = 100\n",
      "\\]\n",
      "\\[\n",
      "4 \\times 5 = 20\n",
      "\\]\n",
      "\\[\n",
      "4 \\times 1 = 4\n",
      "\\]\n",
      "\n",
      "Adding these together:\n",
      "\n",
      "\\[\n",
      "500 + 100 + 20 + 4 = 624\n",
      "\\]\n",
      "\n",
      "Next, convert 624 from decimal to base-7. We do this by dividing by 7 and keeping track of the remainders:\n",
      "\n",
      "\\[\n",
      "624 \\div 7 = 89 \\text{ remainder } 1\n",
      "\\]\n",
      "\\[\n",
      "89 \\div 7 = 12 \\text{ remainder } 5\n",
      "\\]\n",
      "\\[\n",
      "12 \\div 7 = 1 \\text{ remainder } 5\n",
      "\\]\n",
      "\\[\n",
      "1 \\div 7 = 0 \\text{ remainder } 1\n",
      "\\]\n",
      "\n",
      "Reading the remainders from last to first, we get the base-7 number 1551.\n",
      "\n",
      "Thus, the final answer is \\boxed{1551}.\n",
      "\n",
      "Expected Answer:\n",
      "1551\n",
      "\n",
      "Extracted Answer:\n",
      "1551\n",
      "\n",
      "Correct: ✓\n",
      "--------------------------------------------------\n",
      "\n",
      "Response\n",
      "<｜begin_of_sentence｜>  \n",
      "Below is a Math problem that is on the difficulty level of national olympiads. \n",
      "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
      "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
      "although some problems are slightly easier and some are slightly harder. \n",
      "The problems are all in LaTeX format. \n",
      "Answers may require basic computations, e.g., square roots, absolute values. \n",
      "Provide the final answer in the end as a numerical value.\n",
      "This is mandatory and you will be punished for not doing so.\n",
      "Now here is your question: \n",
      "### Problem <｜User｜> Calculate the result of $\\sqrt{{{(-5)}^2}}$. -------------- \n",
      "Provide the answer here: \n",
      "### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜>  \n",
      "The result of $\\sqrt{{{(-5)}^2}}$ is 5.\n",
      "</think>\n",
      "\n",
      "Step 1: Calculate the square of -5.\n",
      "\\[\n",
      "(-5)^2 = (-5) \\times (-5) = 25\n",
      "\\]\n",
      "\n",
      "Step 2: Take the square root of the result.\n",
      "\\[\n",
      "\\sqrt{25} = 5\n",
      "\\]\n",
      "\n",
      "Answer: \\(\\boxed{5}\\)\n",
      "\n",
      "Expected Answer:\n",
      "5\n",
      "\n",
      "Extracted Answer:\n",
      "5\n",
      "\n",
      "Correct: ✓\n",
      "--------------------------------------------------\n",
      "\n",
      "Response\n",
      "<｜begin_of_sentence｜>  \n",
      "Below is a Math problem that is on the difficulty level of national olympiads. \n",
      "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
      "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
      "although some problems are slightly easier and some are slightly harder. \n",
      "The problems are all in LaTeX format. \n",
      "Answers may require basic computations, e.g., square roots, absolute values. \n",
      "Provide the final answer in the end as a numerical value.\n",
      "This is mandatory and you will be punished for not doing so.\n",
      "Now here is your question: \n",
      "### Problem <｜User｜> One student on a field trip counted 12 squirrels. Another counted a third more squirrels than the first student. How many squirrels did both students count combined? -------------- \n",
      "Provide the answer here: \n",
      "### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜>  \n",
      "The first student counted 12 squirrels. The second student counted a third more squirrels than the first student, which means the second student counted 12 + (1/3)*12 = 16 squirrels. Together, both students counted 12 + 16 = 28 squirrels.  \n",
      "The combined total of squirrels counted by both students is 28.  \n",
      "</span>\n",
      "\n",
      "Expected Answer:\n",
      "28\n",
      "\n",
      "Extracted Answer:\n",
      "None\n",
      "\n",
      "Correct: ✗\n",
      "--------------------------------------------------\n",
      "\n",
      "Response\n",
      "<｜begin_of_sentence｜>  \n",
      "Below is a Math problem that is on the difficulty level of national olympiads. \n",
      "You are a international gold medalist in Math, so you should be able to solve this problem. \n",
      "You should expect the difficulty of the problems to be roughly at the level of a national Olympiad, \n",
      "although some problems are slightly easier and some are slightly harder. \n",
      "The problems are all in LaTeX format. \n",
      "Answers may require basic computations, e.g., square roots, absolute values. \n",
      "Provide the final answer in the end as a numerical value.\n",
      "This is mandatory and you will be punished for not doing so.\n",
      "Now here is your question: \n",
      "### Problem <｜User｜> For the past n days, the average (arithmetic mean) daily production at a company was a certain number of units. If today's production of 90 units raises the average to 62 units per day, and the value of n is 14, what was the initial average daily production? -------------- \n",
      "Provide the answer here: \n",
      "### Answer <｜Assistant｜> <think>   answer:  </think> <｜end_of_sentence｜> \n",
      "\n",
      "</think>\n",
      "\n",
      "To determine the initial average daily production, we can follow these steps:\n",
      "\n",
      "1. **Understand the Given Information:**\n",
      "   - The company operated for **n = 14** days.\n",
      "   - The average daily production over these 14 days was **a certain number of units**.\n",
      "   - Today, with a production of **90 units**, the average increased to **62 units per day**.\n",
      "\n",
      "2. **Calculate the Total Production Before Today:**\n",
      "   - The average production over 14 days is **a**, so the total production is:\n",
      "     \\[\n",
      "     \\text{Total production} = a \\times 14\n",
      "     \\]\n",
      "\n",
      "3. **Calculate the Total Production After Today:**\n",
      "   - Today's production is **90 units**, so the new total production is:\n",
      "     \\[\n",
      "     \\text{New total production} = a \\times 14 + 90\n",
      "     \\]\n",
      "   - The new average production over 15 days (14 original days + 1 today) is **62 units**. Thus:\n",
      "     \\[\n",
      "     \\frac{a \\times 14 + 90}{15} = 62\n",
      "     \\]\n",
      "\n",
      "4. **Solve for the Initial Average (a):**\n",
      "   - Multiply both sides by 15 to eliminate the denominator:\n",
      "     \\[\n",
      "     a \\times 14 + 90 = 62 \\times 15\n",
      "     \\]\n",
      "   - Calculate \\(62 \\times 15 = 930\\):\n",
      "     \\[\n",
      "     14a + 90 = 930\n",
      "     \\]\n",
      "   - Subtract 90 from both sides:\n",
      "     \\[\n",
      "     14a = 840\n",
      "     \\]\n",
      "   - Divide both sides by 14:\n",
      "     \\[\n",
      "     a = \\frac{840}{14} = 60\n",
      "     \\]\n",
      "\n",
      "5. **Conclusion:**\n",
      "   - The initial average daily production was **60 units**.\n",
      "\n",
      "**Answer:** 60\n",
      "\n",
      "Expected Answer:\n",
      "60\n",
      "\n",
      "Extracted Answer:\n",
      "None\n",
      "\n",
      "Correct: ✗\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def optimize_model_memory(model):\n",
    "    model.train()\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using primary device: {device}\")\n",
    "\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "output_dir = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-GRPO\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model downloaded\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Detected {num_gpus} GPUs\")\n",
    "device_ids = list(range(num_gpus)) if num_gpus > 1 else None\n",
    "\n",
    "# all_data = train_dataset + test_dataset\n",
    "# random.shuffle(all_data)\n",
    "# size_of_eval_data = 3\n",
    "# eval_data = all_data[:size_of_eval_data]\n",
    "# train_data = all_data[size_of_eval_data:]\n",
    "\n",
    "eval_data = test_dataset\n",
    "train_data = train_dataset\n",
    "\n",
    "print(eval_data[0])\n",
    "\n",
    "print(\"\\nInitial model evaluation before finetuning:\")\n",
    "pre_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Pre-GRPO Accuracy: {pre_grpo_accuracy:.2f}%\")\n",
    "\n",
    "model = optimize_model_memory(model)\n",
    "\n",
    "print(\"\\nStarting RL fine-tuning using GRPO...\")\n",
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 15,\n",
    "    'batch_size': 7,\n",
    "    'num_generations': 6,\n",
    "    'max_completion_length': 200, \n",
    "    'beta': 0.04,\n",
    "    'learning_rate': 5e-6,\n",
    "    'mu': 1,\n",
    "    'epsilon': 0.1\n",
    "}\n",
    "\n",
    "wandb.init(project=os.environ[\"WANDB_PROJECT\"], reinit=True)\n",
    "print(\"Weights & Biases initialized.\")\n",
    "\n",
    "model = train_with_grpo(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_data=train_data,\n",
    "    reward_function=combined_reward,\n",
    "    device_ids=device_ids,\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Training completed and wandb run finished.\")\n",
    "\n",
    "print(\"\\nFinal model evaluation after GRPO RL fine-tunin.g:\")\n",
    "post_grpo_accuracy = evaluate_model(model, tokenizer, eval_data, device)\n",
    "print(f\"Post-GRPO Accuracy: {post_grpo_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86bc78af-2db9-4756-905b-1d20f004a252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving GRPO fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('grpo_finetuned_model/tokenizer_config.json',\n",
       " 'grpo_finetuned_model/special_tokens_map.json',\n",
       " 'grpo_finetuned_model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nSaving GRPO fine-tuned model...\")\n",
    "model.save_pretrained(\"grpo_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"grpo_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac0b71d-e806-4c7e-a284-dc9be799d1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7ff1a-d54e-455e-873c-7868bcfc424e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6aa1c7-935a-46bf-b6e4-2d5c30ac0df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60f8bf-4015-469b-9e44-6396b9450e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04705de-15ef-4fbd-849e-1750830e901e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
